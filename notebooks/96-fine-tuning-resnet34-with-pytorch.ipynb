{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Fine-Tuning with PyTorch\n","\n","Throughout this notebook, we use the <a href='https://www.kaggle.com/c/cifar-10/overview'>CIFAR-10</a> dataset from Kaggle, a popular computer-vision dataset of 60,000 32x32 color images to be classified in one of ten classes, with 6000 images per class. This dataset is complex enough to give a good idea of the benefits of fine-tuning, and why this process can be used to achieve high accuracy scores without spending hours and money to re-train complex models from scratch. With **default hyperparameters** and a medium-sized model (ResNet34), we are able to achieve around **96% accuracy** on Kaggle, 3% off the state-of-the-art for this dataset."]},{"cell_type":"markdown","metadata":{},"source":["# Table of Contents\n","1. [Extracting Data](#extraction)\n","2. [Datasets and DataLoaders](#data)  \n","3. [Training with Validation](#validation)  \n","4. [Full Training](#training)\n","5. [Generating Predictions](#testing) "]},{"cell_type":"markdown","metadata":{},"source":["## Extracting Data <a name=\"extraction\"></a>\n","The original CIFAR-10 dataset provided by Kaggle is composed of two *.7z* files (*train.7z*, *test.7z*), the labels (*trainLabels.csv*) and an example submission (*sampleSubmission.csv*). We provide these files in the *data* folder.  \n","\n","First, we extract the two zipped files in two folers called *original_train* and *original_test*. Then, we move these files to be in the structure required by the `ImageDataset` class of PyTorch, where every image is stored in a folder named as its label (e.g., the nth airplane image will be stored under *airplane/n.png*). We will create four subfolders within *data*:\n","* *train*: contains Training data (excluding Validation set), used to train the model during hyperparameter search\n","* *valid*: contains Validation data, used to train the model during hyperparameter search\n","* *train_valid*: contains Training+Validation data together, used for full re-training of the model\n","* *test* contains Test data, i.e., all the unlabelled data we must submit to Kaggle together with a predicted label"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T07:11:57.614678Z","iopub.status.busy":"2021-08-06T07:11:57.614209Z","iopub.status.idle":"2021-08-06T07:12:11.336270Z","shell.execute_reply":"2021-08-06T07:12:11.335080Z","shell.execute_reply.started":"2021-08-06T07:11:57.614591Z"},"trusted":true},"outputs":[],"source":["# py7zr is required to extract the .7tz files\n","\n","!pip install -q py7zr"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T07:12:13.753920Z","iopub.status.busy":"2021-08-06T07:12:13.753475Z","iopub.status.idle":"2021-08-06T07:27:57.839850Z","shell.execute_reply":"2021-08-06T07:27:57.838716Z","shell.execute_reply.started":"2021-08-06T07:12:13.753885Z"},"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/cifar-10/train.7z'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m root \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m input_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../input/cifar-10\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpy7zr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSevenZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain.7z\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m z:\n\u001b[1;32m      9\u001b[0m     z\u001b[38;5;241m.\u001b[39mextractall(root)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m py7zr\u001b[38;5;241m.\u001b[39mSevenZipFile(input_path\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.7z\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m z:\n","File \u001b[0;32m/opt/conda/envs/bct/lib/python3.11/site-packages/py7zr/py7zr.py:372\u001b[0m, in \u001b[0;36mSevenZipFile.__init__\u001b[0;34m(self, file, mode, filters, dereference, password, header_encryption, blocksize, mp)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(file)\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa   # typeshed issue: 2911\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mopen(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# noqa\u001b[39;00m\n","File \u001b[0;32m/opt/conda/envs/bct/lib/python3.11/pathlib.py:1044\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1043\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/cifar-10/train.7z'"]}],"source":["import py7zr\n","import shutil\n","from pathlib import Path\n","\n","root = Path('./data')\n","input_path = Path('../input/cifar-10')\n","\n","with py7zr.SevenZipFile(input_path/'train.7z', mode='r') as z:\n","    z.extractall(root)\n","\n","with py7zr.SevenZipFile(input_path/'test.7z', mode='r') as z:\n","    z.extractall(root)\n","\n","shutil.copy(input_path/'trainLabels.csv', root/'trainLabels.csv')\n","\n","(root/'train').rename(root/'original_train')\n","(root/'test').rename(root/'original_test')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T07:27:57.842140Z","iopub.status.busy":"2021-08-06T07:27:57.841773Z","iopub.status.idle":"2021-08-06T07:27:57.854900Z","shell.execute_reply":"2021-08-06T07:27:57.853493Z","shell.execute_reply.started":"2021-08-06T07:27:57.842100Z"},"trusted":true},"outputs":[],"source":["from random import random\n","import os\n","\n","def copy_file(source_directory, destination_directory, filename):\n","    \"\"\"\n","    Utility function used to copy a file from a source_directory to a destination_directory\n","    \"\"\"\n","    destination_directory.mkdir(parents=True, exist_ok=True)\n","    shutil.copy(source_directory/filename, destination_directory/filename)\n","    \n","def organize_train_valid_dataset(root, labels, valid_probability=0.1):\n","    \"\"\"\n","    Creates the train, train_valid and valid folders respecting PyTorch's ImageDataset structure, performing\n","    train/validation split based on the given percentage\n","    \"\"\"\n","    source_directory = root/'original_train'\n","    \n","    with os.scandir(source_directory) as it:\n","        for entry in it:\n","            if entry.is_file():\n","                img_index = entry.name.split('.')[0]  # The index is the name of the image except the extension\n","                img_class = labels[labels.id==int(img_index)].label.values[0]  # Find the class by looking up the index in the DF\n","                \n","                # Randomly assign the image to the valid dataset with probability 'valid_probability'\n","                channel = Path('train') if random()>valid_probability else Path('valid')\n","                destination_directory = root/channel/img_class\n","                \n","                # Copy the image to either the train or valid folder, and also to the train_valid folder\n","                copy_file(source_directory, destination_directory, entry.name)\n","                copy_file(source_directory, root/'train_valid'/img_class, entry.name)\n","\n","def organize_test_dataset(root):\n","    \"\"\"\n","    Creates the test folder respecting PyTorch's ImageDataset structure, using a dummy 'undefined' label\n","    \"\"\"\n","    source_directory = root/'original_test'\n","        \n","    with os.scandir(source_directory) as it:\n","        for entry in it:\n","            if entry.is_file():\n","                img_index = entry.name.split('.')[0]  # The index is the name of the image except the extension\n","\n","                channel = Path('test')\n","                destination_directory = root/channel/'undefined'\n","\n","                copy_file(source_directory, destination_directory, entry.name)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T07:27:57.859103Z","iopub.status.busy":"2021-08-06T07:27:57.857967Z","iopub.status.idle":"2021-08-06T07:30:30.997720Z","shell.execute_reply":"2021-08-06T07:30:30.995460Z","shell.execute_reply.started":"2021-08-06T07:27:57.859046Z"},"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'data/trainLabels.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Read in the labels DataFrame with a label for each image\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainLabels.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create the train/train_valid/valid folder structure\u001b[39;00m\n\u001b[1;32m      7\u001b[0m valid_probability \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n","File \u001b[0;32m/opt/conda/envs/bct/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/bct/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m/opt/conda/envs/bct/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/bct/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m/opt/conda/envs/bct/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/trainLabels.csv'"]}],"source":["import pandas as pd\n","\n","# Read in the labels DataFrame with a label for each image\n","labels = pd.read_csv(root/'trainLabels.csv')\n","\n","# Create the train/train_valid/valid folder structure\n","valid_probability = 0.1\n","organize_train_valid_dataset(root, labels, valid_probability)\n","\n","# Create the test folder structure\n","organize_test_dataset(root)"]},{"cell_type":"markdown","metadata":{},"source":["## Datasets and DataLoaders <a name='data'></a>\n","\n","As mentioned above, we rely on the `ImageDataset` class of PyTorch to create the required datasets for training, validation, training+validation and testing. Out of each dataset, we then create a DataLoader to be used in the training/evaluation loops to efficiently fetch images in batches from disk."]},{"cell_type":"markdown","metadata":{},"source":["We perform an initial step to load in the train data and compute the mean and standard deviation of the dataset for each channel (R, G, B), across all images and all pixels. We compute a mean and stdev value batch-by-batch to avoid loading the entire dataset in memory, and then compute the mean of the means and of the stdevs.  \n","**NOTE**: if you have enough RAM (or memory on the GPU), you can use a batch_size equal to the entire train_dataset length, it will provide a more accurate estimation of the means and stdevs by channel."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T07:30:31.012730Z","iopub.status.busy":"2021-08-06T07:30:31.008273Z","iopub.status.idle":"2021-08-06T07:32:14.161735Z","shell.execute_reply":"2021-08-06T07:32:14.160369Z","shell.execute_reply.started":"2021-08-06T07:30:31.012664Z"},"trusted":true},"outputs":[],"source":["!pip install -q --upgrade torchvision"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T07:32:14.164382Z","iopub.status.busy":"2021-08-06T07:32:14.163787Z","iopub.status.idle":"2021-08-06T07:34:10.442974Z","shell.execute_reply":"2021-08-06T07:34:10.441334Z","shell.execute_reply.started":"2021-08-06T07:32:14.164334Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["import torchvision\n","from torchvision.datasets import CIFAR100, CIFAR10\n","import torch\n","\n","transform=torchvision.transforms.Compose([\n","    # Resize step is required as we will use a ResNet model, which accepts at leats 224x224 images\n","    torchvision.transforms.Resize((224,224)),  \n","    torchvision.transforms.ToTensor(),\n","])\n","\n","dataset = torchvision.datasets.CIFAR100(root, train=True, download=True, transform=transform)\n","test_dataset = torchvision.datasets.CIFAR100(root, train=False, download=True, transform=transform)\n","\n","# split train and validation set\n","n_train = int(len(dataset) * 0.8)\n","n_valid = len(dataset) - n_train\n","train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [n_train, n_valid])\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n","\n","means = []\n","stdevs = []\n","for X, _ in train_dataloader:\n","    # Dimensions 0,2,3 are respectively the batch, height and width dimensions\n","    means.append(X.mean(dim=(0,2,3)))\n","    stdevs.append(X.std(dim=(0,2,3)))\n","\n","mean = torch.stack(means, dim=0).mean(dim=0)\n","stdev = torch.stack(stdevs, dim=0).mean(dim=0)"]},{"cell_type":"markdown","metadata":{},"source":["The transforms used for the training and training+validation datasets consist of resizing the images to the required resolution by our ResNet model (224x224), using the `AutoAugment` policy learned on the CIFAR10 dataset and finally converting the image from PIL to Tensor. For the validation and test sets we just resize the image and convert it to Tensor format."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["(tensor([0.5076, 0.4870, 0.4412]), tensor([0.2625, 0.2514, 0.2716]))"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["mean, stdev"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T07:34:10.445657Z","iopub.status.busy":"2021-08-06T07:34:10.445189Z","iopub.status.idle":"2021-08-06T07:34:12.886638Z","shell.execute_reply":"2021-08-06T07:34:12.885463Z","shell.execute_reply.started":"2021-08-06T07:34:10.445591Z"},"trusted":true},"outputs":[],"source":["train_transforms = torchvision.transforms.Compose([\n","        torchvision.transforms.Resize((224,224)),\n","        torchvision.transforms.AutoAugment(policy=torchvision.transforms.AutoAugmentPolicy.CIFAR10),\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Normalize(mean, stdev)\n","    ])\n","\n","train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [n_train, n_valid])\n","\n","\n","valid_transforms = torchvision.transforms.Compose([\n","        torchvision.transforms.Resize((224,224)),\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Normalize(mean, stdev)\n","    ])"]},{"cell_type":"markdown","metadata":{},"source":["The train and train+validation DataLoaders use a smaller `batch_size` as we will also need to keep track of gradients in memory. Furthermore, we shuffle the dataset each epoch to avoid loading the batches in the same order.\n","The valid and test DataLoaders use a larger `batch_size` and do not required to shuffle the dataset as we want deterministic results.\n","\n","The number of workers is generally set to `2 * num_gpus` as a rule of thumb for Kaggle, with `pin_memory = True` to speed up data transfer to the GPU."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T07:34:12.888999Z","iopub.status.busy":"2021-08-06T07:34:12.888525Z","iopub.status.idle":"2021-08-06T07:34:12.902217Z","shell.execute_reply":"2021-08-06T07:34:12.900316Z","shell.execute_reply.started":"2021-08-06T07:34:12.888946Z"},"trusted":true},"outputs":[],"source":["num_gpus = torch.cuda.device_count()\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2*num_gpus, pin_memory=True)\n","# train_valid_dataloader = torch.utils.data.DataLoader(train_valid_dataset, batch_size=128, shuffle=True, num_workers=2*num_gpus, pin_memory=True)\n","\n","valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=256, shuffle=False, num_workers=2*num_gpus, pin_memory=True)\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2*num_gpus, pin_memory=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Training with Validation <a name='validation'></a>\n","\n","The first step of the process is to evaluate the model performance on our own Validation set, consisting of 10% of the labelled data we get from Kaggle. Ideally, this step would be performed while finding the best model and hyperparameters to improve the final accuracy. Here, we just perform this step to show the expected model accuracy before submitting the results to Kaggle.  \n","**NOTE**: when doing proper hyperparameter tuning, depending on the size of the overall labelle data, a k-fold approach might be more appropriate to estimate the generalization capability of the model."]},{"cell_type":"markdown","metadata":{},"source":["We fine-tune a ResNet34 model, trained on ImageNet. Other models might be used, but for the purpose of this notebook a ResNet34 is a good trade-off between training time and model accuracy.  \n","The model originally has a 1000-dimensional output layer, but our dataset has only 10 classes, so we remove the output layer and define a new Fully-Connected layer with just 10 neurons, one for each class in CIFAR-10. The parameters of these new neurons are initialized with Xavier initialization."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T07:34:12.907206Z","iopub.status.busy":"2021-08-06T07:34:12.906359Z","iopub.status.idle":"2021-08-06T07:34:12.914490Z","shell.execute_reply":"2021-08-06T07:34:12.913141Z","shell.execute_reply.started":"2021-08-06T07:34:12.907155Z"},"trusted":true},"outputs":[],"source":["def get_net():\n","    resnet = torchvision.models.resnet34(pretrained=True)\n","    \n","    # Substitute the FC output layer\n","    resnet.fc = torch.nn.Linear(resnet.fc.in_features, 100)\n","    torch.nn.init.xavier_uniform_(resnet.fc.weight)\n","    return resnet"]},{"cell_type":"markdown","metadata":{},"source":["The training loop is a standard PyTorch loop where for every epoch we perform the following macro steps:\n","1. Iterate over the Train DataLoader by making predictions, calculating loss, backpropagating gradients and updating parameters\n","2. Iterate over the Valid DataLoader (if present) to compute the validation loss and accuracy\n","3. Decrease the learning rate using the scheduler (if present)\n","4. Optionally, store the model checkpoint after a given number of `checkpoint_epochs`"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T07:34:12.917457Z","iopub.status.busy":"2021-08-06T07:34:12.916969Z","iopub.status.idle":"2021-08-06T07:34:12.938347Z","shell.execute_reply":"2021-08-06T07:34:12.936783Z","shell.execute_reply.started":"2021-08-06T07:34:12.917411Z"},"trusted":true},"outputs":[],"source":["import time\n","\n","def train(net, train_dataloader, valid_dataloader, criterion, optimizer, scheduler=None, epochs=10, device='cpu', checkpoint_epochs=10):\n","    start = time.time()\n","    print(f'Training for {epochs} epochs on {device}')\n","    \n","    for epoch in range(1,epochs+1):\n","        print(f\"Epoch {epoch}/{epochs}\")\n","        \n","        net.train()  # put network in train mode for Dropout and Batch Normalization\n","        train_loss = torch.tensor(0., device=device)  # loss and accuracy tensors are on the GPU to avoid data transfers\n","        train_accuracy = torch.tensor(0., device=device)\n","        for X, y in train_dataloader:\n","            X = X.to(device)\n","            y = y.to(device)\n","            preds = net(X)\n","            loss = criterion(preds, y)\n","            \n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            \n","            with torch.no_grad():\n","                train_loss += loss * train_dataloader.batch_size\n","                train_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n","        \n","        if valid_dataloader is not None:\n","            net.eval()  # put network in train mode for Dropout and Batch Normalization\n","            valid_loss = torch.tensor(0., device=device)\n","            valid_accuracy = torch.tensor(0., device=device)\n","            with torch.no_grad():\n","                for X, y in valid_dataloader:\n","                    X = X.to(device)\n","                    y = y.to(device)\n","                    preds = net(X)\n","                    loss = criterion(preds, y)\n","\n","                    valid_loss += loss * valid_dataloader.batch_size\n","                    valid_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n","        \n","        if scheduler is not None: \n","            scheduler.step()\n","            \n","        print(f'Training loss: {train_loss/len(train_dataloader.dataset):.2f}')\n","        print(f'Training accuracy: {100*train_accuracy/len(train_dataloader.dataset):.2f}')\n","        \n","        if valid_dataloader is not None:\n","            print(f'Valid loss: {valid_loss/len(valid_dataloader.dataset):.2f}')\n","            print(f'Valid accuracy: {100*valid_accuracy/len(valid_dataloader.dataset):.2f}')\n","        \n","        if epoch%checkpoint_epochs==0:\n","            torch.save({\n","                'epoch': epoch,\n","                'state_dict': net.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","            }, './checkpoint.pth.tar')\n","        \n","        print()\n","    \n","    end = time.time()\n","    print(f'Total training time: {end-start:.1f} seconds')\n","    return net"]},{"cell_type":"markdown","metadata":{},"source":["In this notebook we only use of at most one GPU, you can freely refactor the code to use DistributedDataParallel if you have more GPUs and/or devices.  \n","\n","When fine-tuning, the model parameters of the network body are trained using a lower learning rate than for the head, since for the latter we have to train them from scratch. We rely on Parameter Groups from PyTorch to define two learning rates for the two groups, and use Adam optimizer with `weight_decay = 5e-4` (find via hyperparameter search)."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T07:34:12.940722Z","iopub.status.busy":"2021-08-06T07:34:12.940100Z","iopub.status.idle":"2021-08-06T08:27:42.722397Z","shell.execute_reply":"2021-08-06T08:27:42.720883Z","shell.execute_reply.started":"2021-08-06T07:34:12.940655Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/envs/bct/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/opt/conda/envs/bct/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Training for 20 epochs on cuda\n","Epoch 1/20\n","Training loss: 3.26\n","Training accuracy: 27.61\n","Valid loss: 1.97\n","Valid accuracy: 52.01\n","\n","Epoch 2/20\n","Training loss: 1.44\n","Training accuracy: 62.96\n","Valid loss: 1.27\n","Valid accuracy: 66.61\n","\n","Epoch 3/20\n","Training loss: 0.95\n","Training accuracy: 74.45\n","Valid loss: 1.03\n","Valid accuracy: 71.68\n","\n","Epoch 4/20\n","Training loss: 0.70\n","Training accuracy: 80.99\n","Valid loss: 0.92\n","Valid accuracy: 74.05\n","\n","Epoch 5/20\n","Training loss: 0.53\n","Training accuracy: 85.96\n","Valid loss: 0.86\n","Valid accuracy: 75.40\n","\n","Epoch 6/20\n","Training loss: 0.39\n","Training accuracy: 90.38\n","Valid loss: 0.82\n","Valid accuracy: 76.15\n","\n","Epoch 7/20\n","Training loss: 0.29\n","Training accuracy: 93.80\n","Valid loss: 0.81\n","Valid accuracy: 76.49\n","\n","Epoch 8/20\n","Training loss: 0.21\n","Training accuracy: 96.21\n","Valid loss: 0.80\n","Valid accuracy: 76.63\n","\n","Epoch 9/20\n","Training loss: 0.15\n","Training accuracy: 98.07\n","Valid loss: 0.81\n","Valid accuracy: 76.80\n","\n","Epoch 10/20\n","Training loss: 0.10\n","Training accuracy: 99.11\n","Valid loss: 0.82\n","Valid accuracy: 76.65\n","\n","Epoch 11/20\n","Training loss: 0.07\n","Training accuracy: 99.58\n","Valid loss: 0.82\n","Valid accuracy: 76.94\n","\n","Epoch 12/20\n","Training loss: 0.05\n","Training accuracy: 99.85\n","Valid loss: 0.82\n","Valid accuracy: 77.27\n","\n","Epoch 13/20\n","Training loss: 0.04\n","Training accuracy: 99.94\n","Valid loss: 0.83\n","Valid accuracy: 77.41\n","\n","Epoch 14/20\n","Training loss: 0.03\n","Training accuracy: 99.95\n","Valid loss: 0.85\n","Valid accuracy: 77.12\n","\n","Epoch 15/20\n","Training loss: 0.02\n","Training accuracy: 99.97\n","Valid loss: 0.86\n","Valid accuracy: 77.33\n","\n","Epoch 16/20\n","Training loss: 0.02\n","Training accuracy: 99.97\n","Valid loss: 0.86\n","Valid accuracy: 77.48\n","\n","Epoch 17/20\n","Training loss: 0.01\n","Training accuracy: 99.98\n","Valid loss: 0.87\n","Valid accuracy: 77.62\n","\n","Epoch 18/20\n","Training loss: 0.01\n","Training accuracy: 99.99\n","Valid loss: 0.87\n","Valid accuracy: 77.63\n","\n","Epoch 19/20\n","Training loss: 0.01\n","Training accuracy: 99.99\n","Valid loss: 0.89\n","Valid accuracy: 77.66\n","\n","Epoch 20/20\n","Training loss: 0.01\n","Training accuracy: 99.99\n","Valid loss: 0.89\n","Valid accuracy: 77.59\n","\n","Total training time: 674.0 seconds\n"]}],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","lr, weight_decay, epochs = 1e-5, 5e-4, 20\n","\n","net = get_net().to(device)\n","\n","# Standard CrossEntropy Loss for multi-class classification problems\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# params_1x are the parameters of the network body, i.e., of all layers except the FC layers\n","params_1x = [param for name, param in net.named_parameters() if 'fc' not in str(name)]\n","optimizer = torch.optim.Adam([{'params':params_1x}, {'params': net.fc.parameters(), 'lr': lr*10}], lr=lr, weight_decay=weight_decay)\n","\n","net = train(net, train_dataloader, valid_dataloader, criterion, optimizer, None, epochs, device)"]},{"cell_type":"markdown","metadata":{},"source":["## Full Training <a name='training'></a>\n","After assessing the model performance on the Validation set, we want to re-train the model on the full Training + Validation data to squeeze every performance left before submitting our results to Kaggle. As a general rule, the more data we train on, the better the results will be."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T08:27:42.729443Z","iopub.status.busy":"2021-08-06T08:27:42.728322Z","iopub.status.idle":"2021-08-06T09:23:21.657572Z","shell.execute_reply":"2021-08-06T09:23:21.656387Z","shell.execute_reply.started":"2021-08-06T08:27:42.729385Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'train_valid_dataloader' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m params_1x \u001b[38;5;241m=\u001b[39m [param \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mnamed_parameters() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(name)]\n\u001b[1;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam([{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m:params_1x}, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: net\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: lr\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m}], lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m---> 10\u001b[0m net \u001b[38;5;241m=\u001b[39m train(net, \u001b[43mtrain_valid_dataloader\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m, criterion, optimizer, \u001b[38;5;28;01mNone\u001b[39;00m, epochs, device)\n","\u001b[0;31mNameError\u001b[0m: name 'train_valid_dataloader' is not defined"]}],"source":["lr, weight_decay, epochs = 1e-5, 5e-4, 20\n","\n","net = get_net().to(device)\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","params_1x = [param for name, param in net.named_parameters() if 'fc' not in str(name)]\n","optimizer = torch.optim.Adam([{'params':params_1x}, {'params': net.fc.parameters(), 'lr': lr*10}], lr=lr, weight_decay=weight_decay)\n","\n","net = train(net, train_valid_dataloader, None, criterion, optimizer, None, epochs, device)"]},{"cell_type":"markdown","metadata":{},"source":["## Generating Predictions <a name='testing'></a>\n","After re-training the network on the full labelled dataset, we are ready to score the Test set and submit out results to Kaggle. We iterate over the test_dataloader to get a predicted label for each Test image, and then create a final DataFrame like the one provided in *sampleSubmission.csv* to use on Kaggle."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T09:23:21.659896Z","iopub.status.busy":"2021-08-06T09:23:21.659545Z","iopub.status.idle":"2021-08-06T09:35:38.427546Z","shell.execute_reply":"2021-08-06T09:35:38.426141Z","shell.execute_reply.started":"2021-08-06T09:23:21.659859Z"},"trusted":true},"outputs":[],"source":["preds = []\n","\n","net.eval()\n","with torch.no_grad():\n","    for X, _ in test_dataloader:\n","        X = X.to(device)\n","        preds.extend(net(X).argmax(dim=1).type(torch.int32).cpu().numpy())"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T09:35:38.430040Z","iopub.status.busy":"2021-08-06T09:35:38.429600Z","iopub.status.idle":"2021-08-06T09:35:38.538178Z","shell.execute_reply":"2021-08-06T09:35:38.537075Z","shell.execute_reply.started":"2021-08-06T09:35:38.429994Z"},"trusted":true},"outputs":[],"source":["ids = list(range(1, len(test_dataset)+1))\n","ids.sort(key=lambda x: str(x))"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T09:35:38.540415Z","iopub.status.busy":"2021-08-06T09:35:38.539717Z","iopub.status.idle":"2021-08-06T09:35:39.488100Z","shell.execute_reply":"2021-08-06T09:35:39.486889Z","shell.execute_reply.started":"2021-08-06T09:35:38.540363Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'pd' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: ids, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: preds})\n\u001b[1;32m      2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: train_dataset\u001b[38;5;241m.\u001b[39mclasses[x])\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["df = pd.DataFrame({'id': ids, 'label': preds})\n","df['label'] = df['label'].apply(lambda x: train_dataset.classes[x])\n","df.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
